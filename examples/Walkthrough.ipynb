{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scanner walkthrough\n",
    "\n",
    "To explore how Scanner fits in to a bigger pipeline, we're going to walk through a simple video analysis application. If you want to analyze a film, a common unit of analysis is the _shot_, short segments of video often delineated by the camera cutting to a different angle or location. In this walkthrough, we're going to use Scanner to implement _shot segmentation_, or breaking up a video into shots. To start, we need to get a video. We'll use a scene from Baby Driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T17:43:52.836052Z",
     "start_time": "2019-03-18T17:43:52.822354Z"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video width=\"560\" height=\"315\" src=\"https://storage.googleapis.com/scanner-data/public/sample-clip.mp4?ignore_cache=1\" controls />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We've set up some scripts to help you download the video in the snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T17:45:46.605953Z",
     "start_time": "2019-03-18T17:45:46.052732Z"
    }
   },
   "outputs": [],
   "source": [
    "import util\n",
    "path = util.download_video()\n",
    "print(path)\n",
    "\n",
    "# Read all the frames\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from timeit import default_timer as now\n",
    "\n",
    "print('Reading frames from video...')\n",
    "start = now()\n",
    "video = cv2.VideoCapture(path)\n",
    "frames = []\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret: break\n",
    "    frames.append(frame)\n",
    "print(len(frames))\n",
    "video.release()\n",
    "read_frame_time = now() - start\n",
    "print('Time to read frames: {:.3f}s'.format(read_frame_time))\n",
    "\n",
    "# Display the tenth frame    \n",
    "plt.imshow(cv2.cvtColor(frames[10], cv2.COLOR_RGB2BGR))\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take another look at the video and see if you can identify when shots change. Our shot segmentation algorithm uses the following intuition: in a video, most frames are similar to the one following it. Because most shot changes happen with cuts (as opposed to dissolves or fades), there's an immediate visual break from one frame to the next. We want to identify when the change in visual content between two adjacent frames is substantially larger than normal. One way to estimate change in visual content is by computing a histogram of colors for each frame, i.e. count the number of dark pixels and light pixels in each color channel (red/green/blue), and then compute the magnitude of difference between adjacent frames' histograms. Let's visualize this for the above video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T17:47:34.691849Z",
     "start_time": "2019-03-18T17:47:34.130661Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "histograms = []\n",
    "N = len(frames)\n",
    "\n",
    "# Compute 3 color histograms (one for each channel) for each video frame\n",
    "print('Computing color histograms...')\n",
    "start = now()\n",
    "for frame in tqdm(frames):\n",
    "    hists = [cv2.calcHist([frame], [channel], None, [16], [0, 256]) \n",
    "             for channel in range(3)]\n",
    "    histograms.append(hists)\n",
    "compute_hist_time = now() - start\n",
    "print('Time to compute histograms: {:.3f}s'.format(compute_hist_time))\n",
    "\n",
    "# Compute differences between adjacent pairs of histograms\n",
    "def compute_histogram_diffs(histograms):    \n",
    "    diffs = []        \n",
    "    for i in range(1, N):\n",
    "        frame_diffs = [distance.chebyshev(histograms[i-1][channel], histograms[i][channel]) \n",
    "                       for channel in range(3)]\n",
    "        avg_diff = np.mean(frame_diffs)\n",
    "        diffs.append(avg_diff)\n",
    "    return diffs\n",
    "        \n",
    "diffs = compute_histogram_diffs(histograms)\n",
    "\n",
    "# Plot the differences\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]\n",
    "plt.xlabel(\"Frame number\")\n",
    "plt.ylabel(\"Difference from previous frame\")\n",
    "_ = plt.plot(range(1, N), diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows, for each frame, the difference between its color histograms and the previous frame's color histograms. Try playing around with the number of histogram bins as well as the [distance metric](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html). As you can see, there are a number of sharp peaks interspersed throughout the video that likely correspond to shot boundaries. We can run a sliding window over the above graph to find the peaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T17:47:43.297567Z",
     "start_time": "2019-03-18T17:47:43.252680Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "WINDOW_SIZE = 500  # The size of our sliding window (how many data points to include)\n",
    "OUTLIER_STDDEV = 3 # Outliers are N standard deviations away from the mean of the sliding window\n",
    "\n",
    "def find_shot_boundaries(diffs):\n",
    "    boundaries = []\n",
    "    for i in range(1, N):\n",
    "        window = diffs[max(i-WINDOW_SIZE,0):min(i+WINDOW_SIZE,N)]\n",
    "        if diffs[i-1] - np.mean(window) > OUTLIER_STDDEV * np.std(window):\n",
    "            boundaries.append(i)\n",
    "    return boundaries\n",
    "\n",
    "boundaries = find_shot_boundaries(diffs)        \n",
    "\n",
    "print('Shot boundaries are:')\n",
    "print(boundaries)\n",
    "\n",
    "\n",
    "def tile(imgs, rows=None, cols=None):\n",
    "    # If neither rows/cols is specified, make a square\n",
    "    if rows is None and cols is None:\n",
    "        rows = int(math.sqrt(len(imgs)))\n",
    "\n",
    "    if rows is None:\n",
    "        rows = (len(imgs) + cols - 1) // cols\n",
    "    else:\n",
    "        cols = (len(imgs) + rows - 1) // rows\n",
    "\n",
    "    # Pad missing frames with black\n",
    "    diff = rows * cols - len(imgs)\n",
    "    if diff != 0:\n",
    "        imgs.extend([np.zeros(imgs[0].shape, dtype=imgs[0].dtype) for _ in range(diff)])\n",
    "\n",
    "    return np.vstack([np.hstack(imgs[i * cols:(i + 1) * cols]) for i in range(rows)])\n",
    "\n",
    "\n",
    "montage = tile([frames[i] for i in boundaries])\n",
    "plt.imshow(cv2.cvtColor(montage, cv2.COLOR_RGB2BGR))\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we've done it! The video is now segmented in shots. At this point, you're probably wondering: \"...but I thought this was a Scanner tutorial!\" Well, consider now: what if you wanted to run this pipeline over a second trailer? A movie? A thousand movies? The simple Python code we wrote above is great for experimenting, but doesn't scale. To accelerate this analysis, we need to speed up the core computation, computing the color histogram. Here are some ways we can make that faster:\n",
    "\n",
    "* Use a faster histogram implementation, e.g. using the GPU.\n",
    "* Use a faster video decoder, e.g. the hardware decoder.\n",
    "* Parallelize the histogram pipeline on multiple CPUs or GPUs.\n",
    "* Parallelize the histogram pipeline across a cluster of machines.\n",
    "\n",
    "All of that is fairly difficult to do with Python, but easy with Scanner. \n",
    "\n",
    "Now I'm going to walk you through running the histogram computation in Scanner. First, we start by setting up our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T17:45:48.948917Z",
     "start_time": "2019-03-18T17:45:48.565586Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scannerpy import Client, DeviceType, PerfParams, CacheMode\n",
    "from scannerpy.storage import NamedVideoStream, NamedStream\n",
    "import scannertools.imgproc\n",
    "\n",
    "sc = Client()\n",
    "stream = NamedVideoStream(sc, 'example', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scanner, all data is organized into streams, or lazy lists of elements. Videos are streams where each element is a frame. We can create a stream from a video by defining a `NamedVideoStream` pointing to the video path. The name allows Scanner to store some metadata about the video in a local database that we use to optimize video decode at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T17:46:11.877611Z",
     "start_time": "2019-03-18T17:45:49.179878Z"
    }
   },
   "outputs": [],
   "source": [
    "frame = sc.io.Input([stream])\n",
    "histogram = sc.ops.Histogram(\n",
    "    frame = frame,\n",
    "    device = DeviceType.CPU) # Change this to DeviceType.GPU if you have a GPU\n",
    "output = NamedStream(sc, 'example_hist')\n",
    "output_op = sc.io.Output(sc.streams.Range(histogram, [(0, 2000)]), [output])\n",
    "\n",
    "start = now()\n",
    "sc.run(output_op, PerfParams.estimate(), cache_mode=CacheMode.Overwrite)\n",
    "scanner_time = now() - start\n",
    "print('Time to decode + compute histograms: {:.3f}'.format(scanner_time))\n",
    "print('Scanner was {:.2f}x faster'.format((read_frame_time + compute_hist_time) / scanner_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computations in Scanner are defined in a *data-parallel* manner--that is, you write a computation that takes in one (or a few) frames at a time, and then the Scanner runtime runs your computation in parallel across your video. Here, we define a computation that computes a color histogram for each frame in the video. This is done by defining a series of \"ops\" (operators, similar to TensorFlow):\n",
    "1. The `Input` source represents a stream of frames, the input to our computation. This will be drawn from a video.\n",
    "2. `Histogram` is an op that computes a color histogram over the input `frame`. We specify that it should run on the CPU.\n",
    "3. `Output` represents the final output of our computation, the data that will get written back to disk, in this case a stream containing the histogram for each frame of the input stream.\n",
    "\n",
    "We use `sc.run(...)` with the computation graph (given by the output node) to execute the computation. Next, we want to load the results of our computation into Python for further processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T17:47:54.066708Z",
     "start_time": "2019-03-18T17:47:53.913556Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "histograms = list(output.load())\n",
    "\n",
    "# Run the same shot detection pipeline as before\n",
    "diffs = compute_histogram_diffs(histograms)\n",
    "boundaries = find_shot_boundaries(diffs)\n",
    "montage = tile([frames[i] for i in boundaries])\n",
    "plt.imshow(cv2.cvtColor(montage, cv2.COLOR_RGB2BGR))\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading output is as simple as `output.load()`, a generator that reads elements of the stored stream from disk (or wherever it was written).\n",
    "\n",
    "Let's reflect for a moment on the script we just made. Is it any faster than before? Going back to our four bullet points:\n",
    "\n",
    "* Scanner will run your computation on the GPU (`device=DeviceType.GPU`).\n",
    "* Scanner will use accelerated hardware video decode behind the scenes.\n",
    "* Scanner will automatically run on all of your CPU cores and on multiple GPUs.\n",
    "* Scanner will automatically distribute the work across a cluster.\n",
    "\n",
    "That's what you get for free using Scanner for your video analyses. All of the code for organizing, distributing, and decoding your videos is taken care of by the Scanner runtime. As an exercise, download a long video like a movie and try running both our Python histogram pipeline and the Scanner pipeline. You'll likely notice a substantial difference!\n",
    "\n",
    "So, where should you go from here? I would check out:\n",
    "* [Extended tutorial](https://github.com/scanner-research/scanner/tree/master/examples/tutorial): covers more Scanner features like sampling patterns and building custom ops.\n",
    "* [Example applications](https://github.com/scanner-research/scanner/tree/master/examples/apps): other applications like face detection and reverse image search implemented with Scanner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
